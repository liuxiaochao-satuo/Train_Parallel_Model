# Group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰æ•ˆæœåˆ†æè¯´æ˜

## åˆ†æè„šæœ¬å·¥ä½œåŸç†

### 1. `evaluate_by_group_id.py` æ ¸å¿ƒæµç¨‹

#### æ­¥éª¤1ï¼šåŠ è½½æ ‡æ³¨å¹¶åˆ›å»ºgroup_idæ˜ å°„
```python
# ä»COCOæ ‡æ³¨æ–‡ä»¶ä¸­æå–group_idä¿¡æ¯
coco_gt = COCO(ann_file)
ann_id_to_group_id = {}
for ann_id, ann in coco_gt.anns.items():
    group_id = ann.get('group_id')
    if group_id is not None:
        ann_id_to_group_id[ann_id] = group_id
```

#### æ­¥éª¤2ï¼šè¿‡æ»¤æŒ‡å®šgroup_idçš„æ•°æ®
```python
# è·å–group_id=1çš„æ‰€æœ‰annotation IDsï¼ˆè¿åŠ¨å‘˜ï¼‰
target_ann_ids = [
    ann_id for ann_id in coco_gt.getAnnIds()
    if ann_id_to_group_id.get(ann_id) == 1
]

# è·å–å¯¹åº”çš„image IDs
target_img_ids = set()
for ann_id in target_ann_ids:
    ann = coco_gt.anns[ann_id]
    target_img_ids.add(ann['image_id'])
```

#### æ­¥éª¤3ï¼šåˆ›å»ºè¿‡æ»¤åçš„COCOå¯¹è±¡
```python
filtered_gt = {
    'images': [img for img in coco_gt.dataset['images'] 
               if img['id'] in target_img_ids],
    'annotations': [ann for ann in coco_gt.dataset['annotations'] 
                   if ann['id'] in target_ann_ids],
    'categories': coco_gt.dataset['categories'],
}
```

#### æ­¥éª¤4ï¼šè¿‡æ»¤é¢„æµ‹ç»“æœå¹¶è¯„ä¼°
```python
# åªä¿ç•™ç›®æ ‡å›¾åƒçš„é¢„æµ‹
filtered_pred = [
    pred for pred in pred_data
    if pred['image_id'] in target_img_ids
]

# ä½¿ç”¨COCOevalè¿›è¡Œè¯„ä¼°
coco_eval = COCOeval(filtered_coco_gt, filtered_coco_dt, 'keypoints')
coco_eval.evaluate()
coco_eval.accumulate()
```

#### æ­¥éª¤5ï¼šé‡ç‚¹å±•ç¤ºgroup_id=1çš„ç»“æœ
```python
# é‡ç‚¹å¯¹æ¯”group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰
if 1 in group_ids:
    print("ğŸ¯ é‡ç‚¹åˆ†æï¼šgroup_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰")
    print(f"{'å®éªŒåç§°':<30s} {'AP':>10s} {'AP50':>10s} {'AP75':>10s} {'AR':>10s}")
    
    for exp_name in experiment_names:
        if exp_name in all_results and 1 in all_results[exp_name]:
            r = all_results[exp_name][1]  # group_id=1çš„ç»“æœ
            print(f"{exp_name:<30s} {r['AP']:>10.4f} {r['AP50']:>10.4f} "
                  f"{r['AP75']:>10.4f} {r['AR']:>10.4f}")
    
    # è®¡ç®—æ”¹è¿›å¹…åº¦
    print("\nç›¸å¯¹äºç¬¬ä¸€ä¸ªå®éªŒçš„æ”¹è¿›ï¼ˆgroup_id=1ï¼Œè¿åŠ¨å‘˜ï¼‰:")
    for exp_name in experiment_names[1:]:
        ap = all_results[exp_name][1]['AP']
        improvement = ap - baseline_ap
        improvement_pct = (improvement / baseline_ap) * 100 if baseline_ap > 0 else 0
        arrow = "â†‘" if improvement > 0 else "â†“"
        print(f"  {exp_name:30s} AP: {ap:.4f} "
              f"({arrow}{abs(improvement):.4f}, {arrow}{abs(improvement_pct):.2f}%)")
```

## å¦‚ä½•ä½“ç°group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰çš„æ•ˆæœ

### 1. å•ç‹¬è®¡ç®—APå’ŒAR
- è„šæœ¬ä¼š**å•ç‹¬è®¡ç®—**group_id=1çš„APã€AP50ã€AP75ã€ARç­‰æŒ‡æ ‡
- ä¸åŒ…å«å…¶ä»–group_idçš„æ•°æ®ï¼Œç¡®ä¿ç»“æœåªåæ˜ è¿åŠ¨å‘˜çš„è¯†åˆ«ç²¾åº¦

### 2. ä¸å…¶ä»–group_idå¯¹æ¯”
- åŒæ—¶è®¡ç®—group_id=2ã€3ã€4çš„æŒ‡æ ‡
- å¯ä»¥å¯¹æ¯”ä¸åŒgroup_idä¹‹é—´çš„æ€§èƒ½å·®å¼‚

### 3. å®éªŒé—´å¯¹æ¯”
- å¯¹æ¯”ä¸‰ä¸ªå®éªŒï¼ˆæŸå¤±æƒé‡ã€åŠ æƒé‡‡æ ·ã€ç»„åˆï¼‰åœ¨group_id=1ä¸Šçš„è¡¨ç°
- æ˜¾ç¤ºæ¯ä¸ªå®éªŒå¯¹è¿åŠ¨å‘˜è¯†åˆ«ç²¾åº¦çš„æå‡å¹…åº¦

### 4. æ”¹è¿›å¹…åº¦è®¡ç®—
- è®¡ç®—ç›¸å¯¹äºbaselineï¼ˆç¬¬ä¸€ä¸ªå®éªŒï¼‰çš„ç»å¯¹æå‡å’Œç™¾åˆ†æ¯”æå‡
- ä¾‹å¦‚ï¼š`AP: 0.7456 (â†‘0.0222, â†‘3.07%)` è¡¨ç¤ºAPæå‡äº†0.0222ï¼Œç›¸å¯¹æå‡3.07%

## è¿è¡Œåˆ†æè„šæœ¬

### å‰ææ¡ä»¶
1. éœ€è¦é¢„æµ‹ç»“æœæ–‡ä»¶ï¼ˆ`.keypoints.json`æ ¼å¼ï¼‰
2. é¢„æµ‹æ–‡ä»¶é€šå¸¸åœ¨ï¼š`work_dir/predictions/results.keypoints.json`

### ç”Ÿæˆé¢„æµ‹æ–‡ä»¶
```bash
# ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬çš„testè„šæœ¬
python tools/test_with_fix.py \
    configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w32_parallel_ablation_loss_weight.py \
    work_dirs/ablation_experiments/loss_weight_only/best_coco_AP_epoch_130.pth \
    --work-dir work_dirs/ablation_experiments/loss_weight_only \
    --out work_dirs/ablation_experiments/loss_weight_only/predictions/results.keypoints.json \
    --launcher none
```

### è¿è¡Œåˆ†æ
```bash
python tools/evaluate_by_group_id.py \
    --ann-file /data/lxc/datasets/coco_paralel/annotations_id/person_keypoints_val_parallel.json \
    --work-dirs /data/lxc/outputs/train_parallel_model/ablation_experiments/loss_weight_only \
                /data/lxc/outputs/train_parallel_model/ablation_experiments/weighted_sampling_only \
                /data/lxc/outputs/train_parallel_model/ablation_experiments/combined \
    --experiment-names "Loss Weight Only" "Weighted Sampling Only" "Combined" \
    --group-ids 1 2 3 4
```

## é¢„æœŸè¾“å‡ºç¤ºä¾‹

```
ğŸ¯ é‡ç‚¹åˆ†æï¼šgroup_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰
================================================================================

å®éªŒåç§°                        AP        AP50       AP75         AR
--------------------------------------------------------------------------------
Loss Weight Only            0.7234    0.8567    0.7891    0.8123
Weighted Sampling Only      0.7312    0.8612    0.7956    0.8198
Combined                    0.7456    0.8723    0.8089    0.8345

ç›¸å¯¹äºç¬¬ä¸€ä¸ªå®éªŒçš„æ”¹è¿›ï¼ˆgroup_id=1ï¼Œè¿åŠ¨å‘˜ï¼‰:
  Weighted Sampling Only     AP: 0.7312 (â†‘0.0078, â†‘1.08%)
  Combined                   AP: 0.7456 (â†‘0.0222, â†‘3.07%)
```

## éªŒè¯é›†ç»Ÿè®¡

æ ¹æ®å½“å‰éªŒè¯é›†ï¼š
- **group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰**: 815 ä¸ªæ ‡æ³¨
- group_id=2: 815 ä¸ªæ ‡æ³¨
- group_id=3: 47 ä¸ªæ ‡æ³¨
- group_id=4: 4 ä¸ªæ ‡æ³¨

## å…³é”®æŒ‡æ ‡è¯´æ˜

- **AP (Average Precision)**: å¹³å‡ç²¾åº¦ï¼Œä¸»è¦æŒ‡æ ‡
- **AP50**: OKSé˜ˆå€¼0.5æ—¶çš„AP
- **AP75**: OKSé˜ˆå€¼0.75æ—¶çš„AP
- **AR (Average Recall)**: å¹³å‡å¬å›ç‡

è¿™äº›æŒ‡æ ‡ä¼š**å•ç‹¬è®¡ç®—**group_id=1çš„æ•°æ®ï¼Œç¡®ä¿åªåæ˜ è¿åŠ¨å‘˜çš„è¯†åˆ«æ•ˆæœã€‚

