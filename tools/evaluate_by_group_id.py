#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æŒ‰group_idè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«å…³æ³¨group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰çš„ç²¾åº¦å˜åŒ–

ä½¿ç”¨æ–¹æ³•ï¼š
    # æ–¹æ³•1ï¼šä»work_dirè‡ªåŠ¨æŸ¥æ‰¾é¢„æµ‹ç»“æœ
    python tools/evaluate_by_group_id.py \
        --ann-file data/coco_parallel/annotations_id/person_keypoints_val_parallel.json \
        --work-dirs work_dirs/ablation_experiments/loss_weight_only \
                    work_dirs/ablation_experiments/weighted_sampling_only \
                    work_dirs/ablation_experiments/combined \
        --experiment-names "Loss Weight" "Weighted Sampling" "Combined"
    
    # æ–¹æ³•2ï¼šç›´æ¥æŒ‡å®šé¢„æµ‹ç»“æœæ–‡ä»¶
    python tools/evaluate_by_group_id.py \
        --ann-file data/coco_parallel/annotations_id/person_keypoints_val_parallel.json \
        --pred-files work_dirs/exp1/predictions/results.keypoints.json \
                    work_dirs/exp2/predictions/results.keypoints.json
"""

import json
import argparse
import tempfile
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Optional, Tuple

import numpy as np
from xtcocotools.coco import COCO
from xtcocotools.cocoeval import COCOeval


def find_prediction_file(work_dir: Path) -> Optional[Path]:
    """ä»work_dirä¸­æŸ¥æ‰¾é¢„æµ‹ç»“æœæ–‡ä»¶"""
    # CocoMetricé€šå¸¸å°†ç»“æœä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®ï¼š
    # 1. work_dir/predictions/results.keypoints.json
    # 2. work_dir/checkpoints/epoch_*.keypoints.json
    # 3. work_dir/*.keypoints.json
    
    search_paths = [
        work_dir / 'predictions' / 'results.keypoints.json',
        work_dir / 'predictions' / '*.keypoints.json',
        work_dir / '*.keypoints.json',
    ]
    
    for pattern in search_paths:
        if '*' in str(pattern):
            files = list(work_dir.glob(pattern.name))
            if files:
                # é€‰æ‹©æœ€æ–°çš„
                return max(files, key=lambda x: x.stat().st_mtime)
        else:
            if pattern.exists():
                return pattern
    
    return None


def create_filtered_coco(ann_file: str, group_id: int, output_file: str = None) -> Tuple[COCO, str]:
    """åˆ›å»ºåªåŒ…å«æŒ‡å®šgroup_idçš„COCOå¯¹è±¡
    
    Returns:
        filtered_coco: è¿‡æ»¤åçš„COCOå¯¹è±¡
        temp_file: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½åŸå§‹COCOæ–‡ä»¶
    coco_gt = COCO(ann_file)
    
    # è·å–æŒ‡å®šgroup_idçš„annotation IDs
    target_ann_ids = []
    for ann_id, ann in coco_gt.anns.items():
        if ann.get('group_id') == group_id:
            target_ann_ids.append(ann_id)
    
    if not target_ann_ids:
        return None, None
    
    # è·å–å¯¹åº”çš„image IDs
    target_img_ids = set()
    for ann_id in target_ann_ids:
        ann = coco_gt.anns[ann_id]
        target_img_ids.add(ann['image_id'])
    
    # åˆ›å»ºè¿‡æ»¤åçš„COCOæ•°æ®
    filtered_data = {
        'info': coco_gt.dataset.get('info', {}),
        'licenses': coco_gt.dataset.get('licenses', []),
        'categories': coco_gt.dataset['categories'],
        'images': [img for img in coco_gt.dataset['images'] if img['id'] in target_img_ids],
        'annotations': [ann for ann in coco_gt.dataset['annotations'] if ann['id'] in target_ann_ids],
    }
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    if output_file:
        temp_file = output_file
    else:
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False).name
    
    with open(temp_file, 'w') as f:
        json.dump(filtered_data, f)
    
    # åˆ›å»ºæ–°çš„COCOå¯¹è±¡
    filtered_coco = COCO(temp_file)
    
    return filtered_coco, temp_file


def filter_predictions_by_images(pred_file: str, target_img_ids: set) -> str:
    """è¿‡æ»¤é¢„æµ‹ç»“æœï¼Œåªä¿ç•™ç›®æ ‡å›¾åƒçš„é¢„æµ‹"""
    with open(pred_file, 'r') as f:
        pred_data = json.load(f)
    
    # è¿‡æ»¤é¢„æµ‹ç»“æœ
    filtered_pred = [
        pred for pred in pred_data
        if pred['image_id'] in target_img_ids
    ]
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False).name
    with open(temp_file, 'w') as f:
        json.dump(filtered_pred, f)
    
    return temp_file


def evaluate_group_id(
    ann_file: str,
    pred_file: str,
    group_id: int,
    dataset_meta: dict = None
) -> Dict:
    """è¯„ä¼°æŒ‡å®šgroup_idçš„æ€§èƒ½
    
    Returns:
        åŒ…å«AP, AP50, AP75, ARç­‰æŒ‡æ ‡çš„å­—å…¸
    """
    # åˆ›å»ºè¿‡æ»¤åçš„GT COCOå¯¹è±¡
    filtered_coco_gt, temp_gt_file = create_filtered_coco(ann_file, group_id)
    
    if filtered_coco_gt is None:
        return None
    
    # è·å–ç›®æ ‡å›¾åƒIDs
    target_img_ids = set(filtered_coco_gt.imgs.keys())
    
    # è¿‡æ»¤é¢„æµ‹ç»“æœ
    temp_pred_file = filter_predictions_by_images(pred_file, target_img_ids)
    
    try:
        # åŠ è½½é¢„æµ‹ç»“æœ
        filtered_coco_dt = filtered_coco_gt.loadRes(temp_pred_file)
        
        # è·å–sigmasï¼ˆç”¨äºOKSè®¡ç®—ï¼‰
        if dataset_meta and 'sigmas' in dataset_meta:
            sigmas = np.array(dataset_meta['sigmas'])
        else:
            # é»˜è®¤ä½¿ç”¨COCOçš„sigmasï¼ˆ17ä¸ªå…³é”®ç‚¹ï¼‰
            sigmas = np.array([
                0.026, 0.025, 0.025, 0.035, 0.035, 0.079, 0.079, 0.072, 0.072,
                0.062, 0.062, 0.107, 0.107, 0.087, 0.087, 0.089, 0.089
            ])
            # å¦‚æœæœ‰21ä¸ªå…³é”®ç‚¹ï¼Œéœ€è¦æ‰©å±•
            num_keypoints = len(filtered_coco_gt.loadCats(1)[0]['keypoints'])
            if num_keypoints > 17:
                # æ‰©å±•sigmasï¼ˆæ–°å¢çš„4ä¸ªå…³é”®ç‚¹ä½¿ç”¨ankleçš„sigmaå€¼ï¼‰
                sigmas = np.concatenate([
                    sigmas,
                    np.array([0.089, 0.089, 0.089, 0.089])  # heelå’Œfoot
                ])
        
        # è¿›è¡Œè¯„ä¼°
        coco_eval = COCOeval(filtered_coco_gt, filtered_coco_dt, 'keypoints', sigmas)
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        
        # æå–ç»“æœ
        results = {
            'AP': float(coco_eval.stats[0]),
            'AP50': float(coco_eval.stats[1]),
            'AP75': float(coco_eval.stats[2]),
            'APm': float(coco_eval.stats[3]),  # AP medium
            'APl': float(coco_eval.stats[4]),  # AP large
            'AR': float(coco_eval.stats[5]),
            'AR50': float(coco_eval.stats[6]),
            'AR75': float(coco_eval.stats[7]),
            'ARm': float(coco_eval.stats[8]),
            'ARl': float(coco_eval.stats[9]),
        }
        
        return results
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import os
        if temp_gt_file and os.path.exists(temp_gt_file):
            os.unlink(temp_gt_file)
        if temp_pred_file and os.path.exists(temp_pred_file):
            os.unlink(temp_pred_file)


def analyze_experiments(
    ann_file: str,
    pred_files: List[str],
    experiment_names: List[str],
    group_ids: List[int] = [1, 2, 3, 4],
    dataset_meta: dict = None
):
    """åˆ†æå¤šä¸ªå®éªŒçš„ç»“æœï¼ŒæŒ‰group_idåˆ†ç»„"""
    print("=" * 80)
    print("æŒ‰group_idåˆ†æå®éªŒç»“æœ")
    print("=" * 80)
    print(f"\næ ‡æ³¨æ–‡ä»¶: {ann_file}")
    print(f"å®éªŒæ•°é‡: {len(pred_files)}")
    print(f"åˆ†æçš„group_id: {group_ids}\n")
    
    # åŠ è½½æ ‡æ³¨å¹¶ç»Ÿè®¡group_id
    coco_gt = COCO(ann_file)
    group_id_counts = defaultdict(int)
    
    for ann_id, ann in coco_gt.anns.items():
        group_id = ann.get('group_id')
        if group_id is not None:
            group_id_counts[group_id] += 1
    
    print("Group IDç»Ÿè®¡:")
    for gid in sorted(group_id_counts.keys()):
        print(f"  group_id={gid}: {group_id_counts[gid]} ä¸ªæ ‡æ³¨")
    print()
    
    # å¯¹æ¯ä¸ªå®éªŒè¿›è¡Œè¯„ä¼°
    all_results = {}
    
    for exp_name, pred_file in zip(experiment_names, pred_files):
        if not Path(pred_file).exists():
            print(f"âš ï¸  é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {pred_file}")
            continue
        
        print(f"\n{'='*80}")
        print(f"åˆ†æå®éªŒ: {exp_name}")
        print(f"é¢„æµ‹æ–‡ä»¶: {pred_file}")
        print(f"{'='*80}")
        
        exp_results = {}
        
        for group_id in group_ids:
            if group_id not in group_id_counts:
                print(f"\nè·³è¿‡ group_id={group_id}ï¼ˆæ— æ•°æ®ï¼‰")
                continue
            
            print(f"\nè¯„ä¼° group_id={group_id} ({group_id_counts[group_id]} ä¸ªæ ‡æ³¨)...")
            try:
                results = evaluate_group_id(ann_file, pred_file, group_id, dataset_meta)
                if results:
                    exp_results[group_id] = results
                    print(f"  AP:   {results['AP']:.4f}")
                    print(f"  AP50: {results['AP50']:.4f}")
                    print(f"  AP75: {results['AP75']:.4f}")
                    print(f"  AR:   {results['AR']:.4f}")
            except Exception as e:
                print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        all_results[exp_name] = exp_results
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("å®éªŒç»“æœå¯¹æ¯”ï¼ˆæŒ‰group_idï¼‰")
    print(f"{'='*80}\n")
    
    # é‡ç‚¹å¯¹æ¯”group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰
    if 1 in group_ids:
        print("=" * 80)
        print("ğŸ¯ é‡ç‚¹åˆ†æï¼šgroup_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰")
        print("=" * 80)
        print(f"\n{'å®éªŒåç§°':<30s} {'AP':>10s} {'AP50':>10s} {'AP75':>10s} {'AR':>10s}")
        print("-" * 80)
        
        baseline_ap = None
        for exp_name in experiment_names:
            if exp_name in all_results and 1 in all_results[exp_name]:
                r = all_results[exp_name][1]
                print(f"{exp_name:<30s} {r['AP']:>10.4f} {r['AP50']:>10.4f} "
                      f"{r['AP75']:>10.4f} {r['AR']:>10.4f}")
                if baseline_ap is None:
                    baseline_ap = r['AP']
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        if baseline_ap is not None and len(experiment_names) > 1:
            print(f"\nç›¸å¯¹äºç¬¬ä¸€ä¸ªå®éªŒçš„æ”¹è¿›ï¼ˆgroup_id=1ï¼Œè¿åŠ¨å‘˜ï¼‰:")
            for exp_name in experiment_names[1:]:
                if exp_name in all_results and 1 in all_results[exp_name]:
                    ap = all_results[exp_name][1]['AP']
                    improvement = ap - baseline_ap
                    improvement_pct = (improvement / baseline_ap) * 100 if baseline_ap > 0 else 0
                    arrow = "â†‘" if improvement > 0 else "â†“"
                    print(f"  {exp_name:30s} AP: {ap:.4f} "
                          f"({arrow}{abs(improvement):.4f}, {arrow}{abs(improvement_pct):.2f}%)")
    
    # æ‰€æœ‰group_idçš„è¯¦ç»†å¯¹æ¯”
    print(f"\n{'='*80}")
    print("æ‰€æœ‰group_idçš„è¯¦ç»†å¯¹æ¯”")
    print(f"{'='*80}\n")
    
    for group_id in sorted(group_ids):
        if group_id not in group_id_counts:
            continue
        
        print(f"\ngroup_id={group_id} ({group_id_counts[group_id]} ä¸ªæ ‡æ³¨):")
        print(f"{'å®éªŒåç§°':<30s} {'AP':>10s} {'AP50':>10s} {'AP75':>10s} {'AR':>10s}")
        print("-" * 80)
        
        for exp_name in experiment_names:
            if exp_name in all_results and group_id in all_results[exp_name]:
                r = all_results[exp_name][group_id]
                print(f"{exp_name:<30s} {r['AP']:>10.4f} {r['AP50']:>10.4f} "
                      f"{r['AP75']:>10.4f} {r['AR']:>10.4f}")
    
    return all_results


def main():
    parser = argparse.ArgumentParser(
        description='æŒ‰group_idè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«å…³æ³¨group_id=1ï¼ˆè¿åŠ¨å‘˜ï¼‰')
    parser.add_argument(
        '--ann-file',
        type=str,
        required=True,
        help='COCOæ ¼å¼çš„éªŒè¯é›†æ ‡æ³¨æ–‡ä»¶ï¼ˆåŒ…å«group_idï¼‰')
    parser.add_argument(
        '--pred-files',
        nargs='+',
        type=str,
        default=None,
        help='é¢„æµ‹ç»“æœJSONæ–‡ä»¶åˆ—è¡¨ï¼ˆCOCOæ ¼å¼ï¼Œ.keypoints.jsonï¼‰')
    parser.add_argument(
        '--work-dirs',
        nargs='+',
        type=str,
        default=None,
        help='å®éªŒå·¥ä½œç›®å½•åˆ—è¡¨ï¼ˆä¼šè‡ªåŠ¨æŸ¥æ‰¾é¢„æµ‹æ–‡ä»¶ï¼‰')
    parser.add_argument(
        '--experiment-names',
        nargs='+',
        type=str,
        default=None,
        help='å®éªŒåç§°åˆ—è¡¨')
    parser.add_argument(
        '--group-ids',
        nargs='+',
        type=int,
        default=[1, 2, 3, 4],
        help='è¦åˆ†æçš„group_idåˆ—è¡¨ï¼ˆé»˜è®¤ï¼š[1, 2, 3, 4]ï¼‰')
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    # ç¡®å®šé¢„æµ‹æ–‡ä»¶
    if args.pred_files:
        pred_files = args.pred_files
        if args.experiment_names:
            exp_names = args.experiment_names
        else:
            exp_names = [Path(f).stem for f in pred_files]
    elif args.work_dirs:
        results = []
        for work_dir in args.work_dirs:
            work_path = Path(work_dir)
            exp_name = work_path.name
            pred_file = find_prediction_file(work_path)
            if pred_file:
                results.append((exp_name, str(pred_file)))
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ° {exp_name} çš„é¢„æµ‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡ŒéªŒè¯æˆ–æµ‹è¯•")
        
        if not results:
            print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•é¢„æµ‹æ–‡ä»¶")
            print("\næç¤ºï¼šé¢„æµ‹æ–‡ä»¶é€šå¸¸åœ¨ä»¥ä¸‹ä½ç½®ï¼š")
            print("  - work_dir/predictions/results.keypoints.json")
            print("  - work_dir/*.keypoints.json")
            print("\nè¯·å…ˆè¿è¡ŒéªŒè¯æˆ–æµ‹è¯•ç”Ÿæˆé¢„æµ‹ç»“æœ")
            return
        
        exp_names, pred_files = zip(*results)
        exp_names = list(exp_names)
        pred_files = list(pred_files)
    else:
        print("é”™è¯¯ï¼šå¿…é¡»æä¾› --pred-files æˆ– --work-dirs")
        return
    
    if args.experiment_names:
        if len(args.experiment_names) == len(pred_files):
            exp_names = args.experiment_names
        else:
            print("è­¦å‘Šï¼šå®éªŒåç§°æ•°é‡ä¸é¢„æµ‹æ–‡ä»¶æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤åç§°")
    
    # åŠ è½½æ•°æ®é›†å…ƒä¿¡æ¯ï¼ˆç”¨äºsigmasï¼‰
    try:
        from mmpose.configs._base_.datasets.coco_parallel import dataset_info
        dataset_meta = dataset_info
    except:
        dataset_meta = None
    
    # æ‰§è¡Œåˆ†æ
    results = analyze_experiments(
        ann_file=args.ann_file,
        pred_files=pred_files,
        experiment_names=exp_names,
        group_ids=args.group_ids,
        dataset_meta=dataset_meta
    )
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_path}")


if __name__ == '__main__':
    main()

